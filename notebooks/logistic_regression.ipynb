{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>666163</th>\n",
       "      <td>0</td>\n",
       "      <td>2245183318</td>\n",
       "      <td>Fri Jun 19 15:46:24 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Seductiionn_Ox</td>\n",
       "      <td>So Whats New ? I Died My Hair Last Niqht Im No...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426874</th>\n",
       "      <td>4</td>\n",
       "      <td>2059298361</td>\n",
       "      <td>Sat Jun 06 16:39:01 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>JoyEWil</td>\n",
       "      <td>@MatthewSantos Definitely root for the Cubbies!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782237</th>\n",
       "      <td>0</td>\n",
       "      <td>2323608250</td>\n",
       "      <td>Thu Jun 25 01:02:43 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>dellvink</td>\n",
       "      <td>Sad to hear our tailor in Beijing has closed t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699587</th>\n",
       "      <td>0</td>\n",
       "      <td>2254492614</td>\n",
       "      <td>Sat Jun 20 09:50:01 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>JustinCampbelll</td>\n",
       "      <td>Beingng Lazy &amp;amp; Hungover.....Back Into Wor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145188</th>\n",
       "      <td>0</td>\n",
       "      <td>1882153330</td>\n",
       "      <td>Fri May 22 06:20:13 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>zorp75ck</td>\n",
       "      <td>@wilborne Nope, they are the same age that the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642467</th>\n",
       "      <td>0</td>\n",
       "      <td>2235508642</td>\n",
       "      <td>Fri Jun 19 01:22:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Zaraist</td>\n",
       "      <td>How the hell am i suposed to get Twig Guy's se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16674</th>\n",
       "      <td>0</td>\n",
       "      <td>1556009061</td>\n",
       "      <td>Sat Apr 18 21:21:17 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>pcp071098</td>\n",
       "      <td>@liquilife  Ooops.. I had a bug   But I fixed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576380</th>\n",
       "      <td>0</td>\n",
       "      <td>2211600840</td>\n",
       "      <td>Wed Jun 17 13:03:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Stephanya</td>\n",
       "      <td>@coldfusion1970 I still have a year on my cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146791</th>\n",
       "      <td>0</td>\n",
       "      <td>1882580430</td>\n",
       "      <td>Fri May 22 07:07:35 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>djcraze90</td>\n",
       "      <td>Taking a rest from driving at a citgo. Kristin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518556</th>\n",
       "      <td>4</td>\n",
       "      <td>2175976701</td>\n",
       "      <td>Mon Jun 15 02:33:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ToniHoffmann</td>\n",
       "      <td>@QCDELIVERS sure will, jesus will help</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  \\\n",
       "666163   0  2245183318  Fri Jun 19 15:46:24 PDT 2009  NO_QUERY   \n",
       "1426874  4  2059298361  Sat Jun 06 16:39:01 PDT 2009  NO_QUERY   \n",
       "782237   0  2323608250  Thu Jun 25 01:02:43 PDT 2009  NO_QUERY   \n",
       "699587   0  2254492614  Sat Jun 20 09:50:01 PDT 2009  NO_QUERY   \n",
       "145188   0  1882153330  Fri May 22 06:20:13 PDT 2009  NO_QUERY   \n",
       "...     ..         ...                           ...       ...   \n",
       "642467   0  2235508642  Fri Jun 19 01:22:54 PDT 2009  NO_QUERY   \n",
       "16674    0  1556009061  Sat Apr 18 21:21:17 PDT 2009  NO_QUERY   \n",
       "576380   0  2211600840  Wed Jun 17 13:03:14 PDT 2009  NO_QUERY   \n",
       "146791   0  1882580430  Fri May 22 07:07:35 PDT 2009  NO_QUERY   \n",
       "1518556  4  2175976701  Mon Jun 15 02:33:11 PDT 2009  NO_QUERY   \n",
       "\n",
       "         _TheSpecialOne_  \\\n",
       "666163    Seductiionn_Ox   \n",
       "1426874          JoyEWil   \n",
       "782237          dellvink   \n",
       "699587   JustinCampbelll   \n",
       "145188          zorp75ck   \n",
       "...                  ...   \n",
       "642467           Zaraist   \n",
       "16674          pcp071098   \n",
       "576380         Stephanya   \n",
       "146791         djcraze90   \n",
       "1518556     ToniHoffmann   \n",
       "\n",
       "        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "666163   So Whats New ? I Died My Hair Last Niqht Im No...                                                                   \n",
       "1426874   @MatthewSantos Definitely root for the Cubbies!                                                                    \n",
       "782237   Sad to hear our tailor in Beijing has closed t...                                                                   \n",
       "699587   Beingng Lazy &amp; Hungover.....Back Into Wor ...                                                                   \n",
       "145188   @wilborne Nope, they are the same age that the...                                                                   \n",
       "...                                                    ...                                                                   \n",
       "642467   How the hell am i suposed to get Twig Guy's se...                                                                   \n",
       "16674    @liquilife  Ooops.. I had a bug   But I fixed ...                                                                   \n",
       "576380   @coldfusion1970 I still have a year on my cont...                                                                   \n",
       "146791   Taking a rest from driving at a citgo. Kristin...                                                                   \n",
       "1518556            @QCDELIVERS sure will, jesus will help                                                                    \n",
       "\n",
       "[7000 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = \"cp1252\"\n",
    "encoding1 = \"ISO-8859-1\"\n",
    "df = pd.read_csv('../mi-person/data/sentiment-140-training.1600000.processed.noemoticon.csv', encoding=encoding)\n",
    "\n",
    "X_train = df[:10000]\n",
    "y_train = df[1589999:]\n",
    "X_test = df[10001:20001]\n",
    "y_test = df[30001:40001]\n",
    "\n",
    "df.sample(7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 6)\n",
      "(10000, 6)\n",
      "(10000, 6)\n",
      "(10000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    799999\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/tatchiwiggers/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sentiments \u001b[39m=\u001b[39m SentimentIntensityAnalyzer()\n\u001b[0;32m----> 2\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mPositive\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [sentiments\u001b[39m.\u001b[39mpolarity_scores(i)[\u001b[39m\"\u001b[39m\u001b[39mpos\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m df[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[1;32m      3\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mNegative\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [sentiments\u001b[39m.\u001b[39mpolarity_scores(i)[\u001b[39m\"\u001b[39m\u001b[39mneg\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m df[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[1;32m      4\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mNeutral\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [sentiments\u001b[39m.\u001b[39mpolarity_scores(i)[\u001b[39m\"\u001b[39m\u001b[39mneu\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m df[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]]\n",
      "Cell \u001b[0;32mIn [7], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m sentiments \u001b[39m=\u001b[39m SentimentIntensityAnalyzer()\n\u001b[0;32m----> 2\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mPositive\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [sentiments\u001b[39m.\u001b[39;49mpolarity_scores(i)[\u001b[39m\"\u001b[39m\u001b[39mpos\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m df[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[1;32m      3\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mNegative\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [sentiments\u001b[39m.\u001b[39mpolarity_scores(i)[\u001b[39m\"\u001b[39m\u001b[39mneg\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m df[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[1;32m      4\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mNeutral\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [sentiments\u001b[39m.\u001b[39mpolarity_scores(i)[\u001b[39m\"\u001b[39m\u001b[39mneu\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m df[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/mi-person/lib/python3.8/site-packages/nltk/sentiment/vader.py:361\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.polarity_scores\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39mReturn a float for sentiment strength based on the input text.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[39mPositive values are positive valence, negative value are negative\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[39mvalence.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39m# text, words_and_emoticons, is_cap_diff = self.preprocess(text)\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m sentitext \u001b[39m=\u001b[39m SentiText(\n\u001b[1;32m    362\u001b[0m     text, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstants\u001b[39m.\u001b[39;49mPUNC_LIST, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconstants\u001b[39m.\u001b[39;49mREGEX_REMOVE_PUNCTUATION\n\u001b[1;32m    363\u001b[0m )\n\u001b[1;32m    364\u001b[0m sentiments \u001b[39m=\u001b[39m []\n\u001b[1;32m    365\u001b[0m words_and_emoticons \u001b[39m=\u001b[39m sentitext\u001b[39m.\u001b[39mwords_and_emoticons\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/mi-person/lib/python3.8/site-packages/nltk/sentiment/vader.py:274\u001b[0m, in \u001b[0;36mSentiText.__init__\u001b[0;34m(self, text, punc_list, regex_remove_punctuation)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPUNC_LIST \u001b[39m=\u001b[39m punc_list\n\u001b[1;32m    273\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mREGEX_REMOVE_PUNCTUATION \u001b[39m=\u001b[39m regex_remove_punctuation\n\u001b[0;32m--> 274\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwords_and_emoticons \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_words_and_emoticons()\n\u001b[1;32m    275\u001b[0m \u001b[39m# doesn't separate words from\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[39m# adjacent punctuation (keeps emoticons & contractions)\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_cap_diff \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mallcap_differential(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwords_and_emoticons)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/mi-person/lib/python3.8/site-packages/nltk/sentiment/vader.py:306\u001b[0m, in \u001b[0;36mSentiText._words_and_emoticons\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[39mRemoves leading and trailing puncutation\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[39mLeaves contractions and most emoticons\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39m    Does not preserve punc-plus-letter emoticons (e.g. :D)\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    305\u001b[0m wes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39msplit()\n\u001b[0;32m--> 306\u001b[0m words_punc_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_words_plus_punc()\n\u001b[1;32m    307\u001b[0m wes \u001b[39m=\u001b[39m [we \u001b[39mfor\u001b[39;00m we \u001b[39min\u001b[39;00m wes \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(we) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[1;32m    308\u001b[0m \u001b[39mfor\u001b[39;00m i, we \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(wes):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/mi-person/lib/python3.8/site-packages/nltk/sentiment/vader.py:293\u001b[0m, in \u001b[0;36mSentiText._words_plus_punc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m words_only \u001b[39m=\u001b[39m {w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m words_only \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(w) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m}\n\u001b[1;32m    292\u001b[0m \u001b[39m# the product gives ('cat', ',') and (',', 'cat')\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m punc_before \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(p): p[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m product(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPUNC_LIST, words_only)}\n\u001b[1;32m    294\u001b[0m punc_after \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(p): p[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m product(words_only, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPUNC_LIST)}\n\u001b[1;32m    295\u001b[0m words_punc_dict \u001b[39m=\u001b[39m punc_before\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/mi-person/lib/python3.8/site-packages/nltk/sentiment/vader.py:293\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    291\u001b[0m words_only \u001b[39m=\u001b[39m {w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m words_only \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(w) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m}\n\u001b[1;32m    292\u001b[0m \u001b[39m# the product gives ('cat', ',') and (',', 'cat')\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m punc_before \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(p): p[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m product(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPUNC_LIST, words_only)}\n\u001b[1;32m    294\u001b[0m punc_after \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(p): p[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m product(words_only, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPUNC_LIST)}\n\u001b[1;32m    295\u001b[0m words_punc_dict \u001b[39m=\u001b[39m punc_before\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sentiments = SentimentIntensityAnalyzer()\n",
    "df[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in df[\"text\"]]\n",
    "df[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in df[\"text\"]]\n",
    "df[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in df[\"text\"]]\n",
    "df['Compound'] = [sentiments.polarity_scores(i)[\"compound\"] for i in df[\"text\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = df[\"Compound\"].values\n",
    "sentiment = []\n",
    "for i in score:\n",
    "    if i >= 0.05:\n",
    "        sentiment.append('Positive')\n",
    "    elif i <= -0.05:\n",
    "        sentiment.append('Negative')\n",
    "    else:\n",
    "        sentiment.append('Neutral')\n",
    "df[\"Sentiment\"] = sentiment\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/tatchiwiggers/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# RegEx for removing non-letter characters\n",
    "import re\n",
    "\n",
    "# NLTK library for the remaining steps\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")   # download list of stopwords (only once; need not run it again)\n",
    "from nltk.corpus import stopwords # import stopwords\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msg_to_words(string):\n",
    "    \"\"\"Convert string into a sequence of words.\"\"\"\n",
    "\n",
    "    string = re.sub(r\"[^a-zA-Z0-9]\", \" \", string.lower())\n",
    "    words = string.split()\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    words = [PorterStemmer().stem(w) for w in words]\n",
    "\n",
    "    # Return final list of words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joanneteh', 'grumpi', 'new', 'comput', 'work']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_to_words(\"\"\"@joanneteh We all have grumpiness. New computer = working\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 44\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[39mreturn\u001b[39;00m words_train, words_test, labels_train, labels_test\n\u001b[1;32m     42\u001b[0m \u001b[39m# Preprocess data\u001b[39;00m\n\u001b[1;32m     43\u001b[0m words_train, words_test, labels_train, labels_test \u001b[39m=\u001b[39m preprocess_data(\n\u001b[0;32m---> 44\u001b[0m         data_train, data_test, labels_train, labels_test)\n\u001b[1;32m     46\u001b[0m \u001b[39m# Take a look at a sample\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m--- Raw review ---\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_train' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"cache\", \"sentiment_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        words_train = list(map(msg_to_words, data_train))\n",
    "        words_test = list(map(msg_to_words, data_test))\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test\n",
    "\n",
    "\n",
    "# Preprocess data\n",
    "words_train, words_test, labels_train, labels_test = preprocess_data(\n",
    "        data_train, data_test, labels_train, labels_test)\n",
    "\n",
    "# Take a look at a sample\n",
    "print(\"\\n--- Raw review ---\")\n",
    "print(data_train[1])\n",
    "print(\"\\n--- Preprocessed words ---\")\n",
    "print(words_train[1])\n",
    "print(\"\\n--- Label ---\")\n",
    "print(labels_train[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('mi-person')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad8f911216fb44df1a6bdf3bc43e179ecc09f87b6d0815550788f565bf5d24f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
