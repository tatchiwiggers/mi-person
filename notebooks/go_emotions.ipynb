{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../mi-person/data/go_emotions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>example_very_unclear</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>207674</th>\n",
       "      <td>eetgibp</td>\n",
       "      <td>BT In the commentary booth: \"Where's your god ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105463</th>\n",
       "      <td>ee6w1dq</td>\n",
       "      <td>They'll make them walk the plank. Granny will ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33124</th>\n",
       "      <td>eeeec7k</td>\n",
       "      <td>That's how it be sometimes.</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95846</th>\n",
       "      <td>edu2qc9</td>\n",
       "      <td>Looks like your sources were closer to the act...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107394</th>\n",
       "      <td>ed3792u</td>\n",
       "      <td>I feel this!</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103805</th>\n",
       "      <td>ed1wkw8</td>\n",
       "      <td>Article is good. Not sure how it relates to “c...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29508</th>\n",
       "      <td>ef2n7oz</td>\n",
       "      <td>The only thing wrong with her is her face. May...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66059</th>\n",
       "      <td>edffuj3</td>\n",
       "      <td>This team is fucking embarrassing.</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126535</th>\n",
       "      <td>eeseie5</td>\n",
       "      <td>Hes looking for a handout of 3 free meals a da...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140402</th>\n",
       "      <td>ef9yd9k</td>\n",
       "      <td>I care too much about what others think. Const...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "207674  eetgibp  BT In the commentary booth: \"Where's your god ...   \n",
       "105463  ee6w1dq  They'll make them walk the plank. Granny will ...   \n",
       "33124   eeeec7k                        That's how it be sometimes.   \n",
       "95846   edu2qc9  Looks like your sources were closer to the act...   \n",
       "107394  ed3792u                                       I feel this!   \n",
       "...         ...                                                ...   \n",
       "103805  ed1wkw8  Article is good. Not sure how it relates to “c...   \n",
       "29508   ef2n7oz  The only thing wrong with her is her face. May...   \n",
       "66059   edffuj3                 This team is fucking embarrassing.   \n",
       "126535  eeseie5  Hes looking for a handout of 3 free meals a da...   \n",
       "140402  ef9yd9k  I care too much about what others think. Const...   \n",
       "\n",
       "        example_very_unclear  admiration  amusement  anger  annoyance  \\\n",
       "207674                 False           0          0      0          0   \n",
       "105463                 False           0          0      0          0   \n",
       "33124                   True           0          0      0          0   \n",
       "95846                  False           0          0      0          0   \n",
       "107394                 False           0          0      0          0   \n",
       "...                      ...         ...        ...    ...        ...   \n",
       "103805                 False           0          0      0          0   \n",
       "29508                  False           0          0      0          0   \n",
       "66059                  False           0          0      1          1   \n",
       "126535                 False           0          0      0          0   \n",
       "140402                 False           0          0      0          0   \n",
       "\n",
       "        approval  caring  confusion  ...  love  nervousness  optimism  pride  \\\n",
       "207674         0       0          0  ...     0            0         0      0   \n",
       "105463         1       0          0  ...     0            0         0      0   \n",
       "33124          0       0          0  ...     0            0         0      0   \n",
       "95846          0       0          0  ...     0            0         0      0   \n",
       "107394         0       0          0  ...     0            0         0      0   \n",
       "...          ...     ...        ...  ...   ...          ...       ...    ...   \n",
       "103805         0       0          1  ...     0            0         0      0   \n",
       "29508          0       0          0  ...     0            0         0      0   \n",
       "66059          0       0          0  ...     0            0         0      0   \n",
       "126535         0       0          0  ...     0            0         0      0   \n",
       "140402         0       0          0  ...     0            0         0      0   \n",
       "\n",
       "        realization  relief  remorse  sadness  surprise  neutral  \n",
       "207674            0       0        0        0         0        1  \n",
       "105463            0       0        0        0         0        0  \n",
       "33124             0       0        0        0         0        0  \n",
       "95846             0       0        0        0         0        1  \n",
       "107394            0       0        0        0         0        1  \n",
       "...             ...     ...      ...      ...       ...      ...  \n",
       "103805            0       0        0        0         0        0  \n",
       "29508             0       0        0        0         0        0  \n",
       "66059             0       0        0        0         0        0  \n",
       "126535            0       0        0        0         0        1  \n",
       "140402            0       0        0        0         0        0  \n",
       "\n",
       "[7000 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "df.sample(7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9245, 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df[df['amusement'] == 1]\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop(columns=['id', 'text', 'example_very_unclear'])\n",
    "df_concat = pd.DataFrame()\n",
    "for column in df2.columns:\n",
    "    df_temp = df[df[column] == 1].sample(673)\n",
    "    \n",
    "    df_concat = pd.concat([df_concat, df_temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18844, 31)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9826</th>\n",
       "      <td>I had to buy all your music after seeing that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92911</th>\n",
       "      <td>Good job ignoring [NAME].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150621</th>\n",
       "      <td>That would be very cool and a nice non-HP bar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199404</th>\n",
       "      <td>I love Caribbean food! I'm adding this one to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75627</th>\n",
       "      <td>[NAME] looking like [NAME] out there</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "9826    I had to buy all your music after seeing that ...\n",
       "92911                           Good job ignoring [NAME].\n",
       "150621  That would be very cool and a nice non-HP bar ...\n",
       "199404  I love Caribbean food! I'm adding this one to ...\n",
       "75627                [NAME] looking like [NAME] out there"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df_concat[['text']]\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def __remove_punctuation(text):\n",
    "    \"\"\"\n",
    "        remove punctuation from text and lower case it\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "\n",
    "    punctuations = string.punctuation\n",
    "    punctuations += '“'\n",
    "    punctuations += '’'\n",
    "    punctuations += '”'\n",
    "    punctuations += '’'\n",
    "    punctuations += ' — '\n",
    "    punctuations += 'â€œ'\n",
    "    punctuations += 'â€¦'\n",
    "    punctuations += 'â€'\n",
    "    punctuations += '€™'\n",
    "    punctuations += '€'\n",
    "    punctuations += '™'\n",
    "    punctuations += '¦'\n",
    "    punctuations += 'œ'\n",
    "    punctuations += 'Â'\n",
    "    punctuations += 'Ã'\n",
    "    punctuations += '— '\n",
    "    punctuations += '¶'\n",
    "    punctuations += '§'\n",
    "    punctuations += '£'\n",
    "    punctuations += '©'\n",
    "    punctuations += 'ª'\n",
    "    punctuations += '³'\n",
    "\n",
    "    # text = emoji.get_emoji_regexp().sub(u'', text)\n",
    "\n",
    "    for punctuation in punctuations:\n",
    "        text = text.replace(punctuation, ' ') \n",
    "        #text = text.replace('donald', 'trump')\n",
    "        #text = text.replace('clinton', 'hillary')\n",
    "    return text.lower() # lower case\n",
    "\n",
    "def __remove_numbers(text):\n",
    "    \"\"\"\n",
    "        remove number from text\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "\n",
    "    words_only = ''.join([i for i in text if not i.isdigit()])\n",
    "    return words_only\n",
    "\n",
    "def __remove_stopwords(text):\n",
    "    \"\"\"\n",
    "        remove stop words from text\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    #stop_words += stopwords.words('portuguese')\n",
    "    stop_words.append('mr')\n",
    "    stop_words = set(stop_words)\n",
    "\n",
    "    tokenized = word_tokenize(text)\n",
    "    without_stopwords = [word for word in tokenized if not word in stop_words]\n",
    "    return without_stopwords\n",
    "\n",
    "def __lemmatize(text):\n",
    "    \"\"\"\n",
    "        lemmatize text\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in text]\n",
    "    lemmatized_string = \" \".join(lemmatized)\n",
    "    return lemmatized_string\n",
    "\n",
    "\n",
    "def process_data(df):\n",
    "    \"\"\"\n",
    "        process the data\n",
    "    \"\"\"\n",
    "\n",
    "    df_ = df.copy()\n",
    "        \n",
    "    df_['text'] = df_['text'].apply(__remove_punctuation)\n",
    "\n",
    "    df_['text'] = df_['text'].apply(__remove_numbers)\n",
    "\n",
    "    df_['text'] = df_['text'].apply(__remove_stopwords)\n",
    "\n",
    "    df_['text'] = df_['text'].apply(__lemmatize)\n",
    "    \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['buy music seeing vid amazing work'],\n",
       "       ['good job ignoring name'],\n",
       "       ['would cool nice non hp bar visualization'],\n",
       "       ...,\n",
       "       ['actually carrot'],\n",
       "       ['ok dear'],\n",
       "       ['one writer twitter really']], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentences = process_data(X_train).to_numpy()\n",
    "cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tf_idf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "# processed_features = [tf_idf_vectorizer.fit_transform(cleaned_sentences) for sentence\n",
    "#                       in cleaned_sentences]\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted_words = pd.DataFrame(tf_idf_vectorizer.fit_transform(processed_features).toarray(),\n",
    "#                  columns = tf_idf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# weighted_words\n",
    "import re\n",
    "\n",
    "def clean_data(text):\n",
    "    text = re.sub(r'[^ \\nA-Za-z0-9À-ÖØ-öø-ÿ/]+', '', text)\n",
    "    text = re.sub(r'[\\\\/×\\^\\]\\[÷]', '', text)\n",
    "    return text\n",
    "\n",
    "def change_lower(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "def remover(text):\n",
    "    text_tokens = text.split(\" \")\n",
    "    final_list = [word for word in text_tokens if not word in stopwords_list]\n",
    "    text = ' '.join(final_list)\n",
    "    return text\n",
    "\n",
    "def get_w2vdf(df):\n",
    "    w2v_df = pd.DataFrame(df[\"text\"]).values.tolist()\n",
    "    for i in range(len(w2v_df)):\n",
    "        w2v_df[i] = w2v_df[i][0].split(\" \")\n",
    "    return w2v_df\n",
    "\n",
    "def train_w2v(w2v_df):\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    w2v_model = Word2Vec(min_count=4,\n",
    "                         window=4,\n",
    "                         alpha=0.03, \n",
    "                         min_alpha=0.0007, \n",
    "                         sg = 1,\n",
    "                         workers=cores-1)\n",
    "    \n",
    "    w2v_model.build_vocab(w2v_df, progress_per=10000)\n",
    "    w2v_model.train(w2v_df, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1)\n",
    "    return w2v_model\n",
    "\n",
    "df[[\"text\"]] = df[[\"text\"]].astype(str)\n",
    "df[\"text\"] = df[\"text\"].apply(change_lower)\n",
    "df[\"text\"] = df[\"text\"].apply(clean_data)\n",
    "df[\"text\"] = df[\"text\"].apply(remover)\n",
    "\n",
    "w2v_df = get_w2vdf(df)\n",
    "w2v_model = train_w2v(w2v_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zzzzzz', 0.5652974247932434),\n",
       " ('masked', 0.5458847880363464),\n",
       " ('hentaipoon', 0.5437642335891724),\n",
       " ('awesome', 0.5395288467407227),\n",
       " ('2cb', 0.5388690829277039),\n",
       " ('amazing', 0.5319364070892334),\n",
       " ('belgrade', 0.5256741642951965),\n",
       " ('margarine', 0.524250328540802),\n",
       " ('good', 0.5152955055236816),\n",
       " ('glutenfrei', 0.5046489238739014)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_df\n",
    "w2v_model.wv.most_similar(positive=[\"great\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('incontinence', 0.628472626209259),\n",
       " ('debater', 0.6074535250663757),\n",
       " ('favouritism', 0.5826535224914551),\n",
       " ('teamlemoncreams', 0.5520726442337036),\n",
       " ('aggregate', 0.5432706475257874),\n",
       " ('plohamas', 0.5418367385864258),\n",
       " ('materials', 0.5376135110855103),\n",
       " ('moods', 0.5249935984611511),\n",
       " ('exploration', 0.5169201493263245),\n",
       " ('ballparks', 0.5020651817321777)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"terrible\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('10000th', 0.5764591693878174),\n",
       " ('alds', 0.5188714861869812),\n",
       " ('1682', 0.4755295515060425),\n",
       " ('virginia', 0.4706263840198517),\n",
       " ('fatigued', 0.470175176858902),\n",
       " ('carolina', 0.46961137652397156),\n",
       " ('location', 0.4668193757534027),\n",
       " ('av', 0.462760329246521),\n",
       " ('1980', 0.45806530117988586),\n",
       " ('harvard', 0.4501142203807831)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"boston\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('edmunds', 0.6769403219223022),\n",
       " ('uglier', 0.6532434821128845),\n",
       " ('plaw', 0.62839674949646),\n",
       " ('degend', 0.6231839060783386),\n",
       " ('kets', 0.5727165341377258),\n",
       " ('wuf', 0.5635632872581482),\n",
       " ('brandy', 0.5564343929290771),\n",
       " ('intersepts', 0.5503106713294983),\n",
       " ('crating', 0.545530378818512),\n",
       " ('undeniable', 0.5446431636810303)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"dog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7f5d094af850>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(w2v_df, min_count=1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18844, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = df_concat.drop(columns=['id', 'text', 'example_very_unclear'])\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = OneVsRestClassifier(LogisticRegression()).fit(processed_features, y_train)\n",
    "# model = OneVsRestClassifier(LogisticRegression()).fit(processed_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You do right, if you don't care then fuck 'em!\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [94], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m processed_test \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mtransform(df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues)\n\u001b[0;32m----> 2\u001b[0m prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(processed_test)\n\u001b[1;32m      3\u001b[0m prediction_proba \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_proba(processed_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "processed_test = vectorizer.transform(df['text'].values)\n",
    "prediction = model.predict(processed_test)\n",
    "prediction_proba = model.predict_proba(processed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5879333545366263\n",
      "4.139329719007467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(prediction[0])\n",
    "print(prediction_proba[0].sum())\n",
    "print(prediction_proba[0][np.argmax(prediction[2])]*100)\n",
    "np.argmax(prediction[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {\"text\": [\"that game hurt\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = pd.DataFrame.from_dict(data)\n",
    "# test_vec = vectorizer.transform(X_test['text'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(processed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44739292, 0.03586595, 0.02809953, ..., 0.02414154, 0.02709744,\n",
       "        0.03835104],\n",
       "       [0.058321  , 0.24651841, 0.06306535, ..., 0.05902671, 0.02587264,\n",
       "        0.04027202],\n",
       "       [0.10469759, 0.0209913 , 0.05397462, ..., 0.03384042, 0.02801978,\n",
       "        0.04331961],\n",
       "       ...,\n",
       "       [0.04203796, 0.04283444, 0.0555304 , ..., 0.03986793, 0.01540153,\n",
       "        0.09163799],\n",
       "       [0.10061908, 0.03004642, 0.04698681, ..., 0.02987135, 0.01896844,\n",
       "        0.0306477 ],\n",
       "       [0.04314211, 0.03290709, 0.04596256, ..., 0.0680064 , 0.04222355,\n",
       "        0.11853829]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(processed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.99724050095521"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(processed_features, y_train)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('mi-person')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad8f911216fb44df1a6bdf3bc43e179ecc09f87b6d0815550788f565bf5d24f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
